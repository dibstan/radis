x-app: &default-app
  image: radis_prod:latest
  env_file:
    - ../.env.prod
  environment:
    ENABLE_REMOTE_DEBUGGING: 0
    DJANGO_SETTINGS_MODULE: "radis.settings.production"
    SSL_CERT_FILE: "/var/www/web/ssl/cert.pem"
    SSL_KEY_FILE: "/var/www/web/ssl/key.pem"

x-deploy: &deploy
  replicas: 1
  restart_policy:
    condition: on-failure
    max_attempts: 3

services:
  # We can't use those manage commands inside the web container in production because
  # the web service may have multiple replicas. So we make sure to only run them once
  # and wait for it to be finished by the web service containers.
  init:
    <<: *default-app
    hostname: init.local
    command: >
      bash -c "
        wait-for-it -s postgres.local:5432 -t 120 && 
        ./manage.py migrate &&
        ./manage.py collectstatic --no-input &&
        ./manage.py create_admin &&
        ./manage.py generate_cert &&
        ./manage.py ok_server --host 0.0.0.0 --port 8000
      "
    deploy:
      <<: *deploy

  web:
    <<: *default-app
    build:
      target: production
    ports:
      - "${RADIS_HTTP_PORT:-80}:80"
      - "${RADIS_HTTPS_PORT:-443}:443"
    command: >
      bash -c "
        wait-for-it -s init.local:8000 -t 300 &&
        echo 'Starting web server ...'
        daphne -b 0.0.0.0 -p 80 -e ssl:443:privateKey=/var/www/web/ssl/key.pem:certKey=/var/www/web/ssl/cert.pem radis.asgi:application
      "
    deploy:
      <<: *deploy
      replicas: 3

  worker_default:
    <<: *default-app
    command: >
      bash -c "
        wait-for-it -s postgres.local:5432 -t 60 &&
        ./manage.py bg_worker -q default
      "
    deploy:
      <<: *deploy

  worker_llm:
    <<: *default-app
    command: >
      bash -c "
        wait-for-it -s postgres.local:5432 -t 60 &&
        ./manage.py bg_worker -q llm
      "
    deploy:
      <<: *deploy

  llamacpp_gpu:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    hostname: llamacpp.local
    environment:
      LLAMA_CACHE: "/models"
    env_file:
      - ../.env.prod
    ports:
      - 9610:8080
    volumes:
      - models_data:/models
    entrypoint: "/bin/bash -c '/llama-server -mu $${LLM_MODEL_URL} -ngl 99 -cb -c 4096 --host 0.0.0.0 --port 8080'"
    deploy:
      # <<: *deploy
      resources:
        reservations:
          # https://gist.github.com/medihack/6a6d24dc6376939e1919f32409c2119f
          generic_resources:
            - discrete_resource_spec:
                kind: "gpu"
                value: 1

  postgres:
    env_file:
      - ../.env.prod
    deploy:
      <<: *deploy

volumes:
  models_data:
